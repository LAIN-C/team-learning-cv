{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCBAGpD4T73Y"
      },
      "source": [
        "baseline思路：使用CNN进行定长字符分类；\n",
        "\n",
        "运行系统要求：Python2/3，内存4G，有无GPU都可以"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.下载街景字符识别的数据集，并且解压\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import shutil\n",
        "import shutil\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!ls -l /content/drive/MyDrive\n",
        "\n",
        "links = pd.read_csv('/content/drive/MyDrive/mchar_data_list_0515.csv')\n",
        "dir_name = 'NDataset'\n",
        "mypath = '/content/'\n",
        "if not os.path.exists(mypath + dir_name):\n",
        "    os.mkdir(mypath + dir_name)\n",
        "for i,link in enumerate(links['link']):\n",
        "    file_name = links['file'][i]\n",
        "    print(file_name, '\\t', link)\n",
        "    file_name = mypath + dir_name + '/' + file_name\n",
        "    if not os.path.exists(file_name):\n",
        "        response = requests.get(link, stream=True)\n",
        "        with open( file_name, 'wb') as f:\n",
        "            for chunk in response.iter_content(chunk_size=1024):\n",
        "                if chunk:\n",
        "                    f.write(chunk)\n",
        "zip_list = ['mchar_train', 'mchar_test_a', 'mchar_val']\n",
        "for little_zip in zip_list:\n",
        "    if not os.path.exists(mypath + dir_name + '/' + little_zip):\n",
        "        zip_file = zipfile.ZipFile(mypath + dir_name + '/' + little_zip + '.zip', 'r')\n",
        "        zip_file.extractall(path = mypath + dir_name )\n",
        "if os.path.exists(mypath + dir_name + '/' + '__MACOSX'):\n",
        "    shutil.rmtree(mypath + dir_name + '/' + '__MACOSX')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JpDakhdYV8cP",
        "outputId": "521c19c6-09d1-435f-c3e4-f897b7667f96"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "total 153553\n",
            "drwx------ 2 root root      4096 Feb 27 15:05 'Colab Notebooks'\n",
            "drwx------ 2 root root      4096 Mar 22 07:35  mchar\n",
            "-rw------- 1 root root       659 Mar 22 07:35  mchar_data_list_0515.csv\n",
            "-rw------- 1 root root 141581580 Mar 19 18:19  model_best.pth\n",
            "-rw------- 1 root root  15646679 Mar 19 17:25  tmp.csv\n",
            "mchar_train.zip \t http://tianchi-competition.oss-cn-hangzhou.aliyuncs.com/531795/mchar_train.zip\n",
            "mchar_train.json \t http://tianchi-competition.oss-cn-hangzhou.aliyuncs.com/531795/mchar_train.json\n",
            "mchar_val.zip \t http://tianchi-competition.oss-cn-hangzhou.aliyuncs.com/531795/mchar_val.zip\n",
            "mchar_val.json \t http://tianchi-competition.oss-cn-hangzhou.aliyuncs.com/531795/mchar_val.json\n",
            "mchar_test_a.zip \t http://tianchi-competition.oss-cn-hangzhou.aliyuncs.com/531795/mchar_test_a.zip\n",
            "mchar_sample_submit_A.csv \t http://tianchi-competition.oss-cn-hangzhou.aliyuncs.com/531795/mchar_sample_submit_A.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.定义目录路径\n",
        "data_dir = {\n",
        "    'train_data': '/content/NDataset/mchar_train/',\n",
        "    'val_data': '/content/NDataset/mchar_val/',\n",
        "    'test_data': '/content/NDataset/mchar_test_a/',\n",
        "    'train_label': '/content/NDataset/mchar_train.json',\n",
        "    'val_label': '/content/NDataset/mchar_val.json',\n",
        "    'submit_file': '/content/NDataset/mchar_sample_submit_A.csv'\n",
        "}\n",
        "#3.统计train,val,test数据集的个数\n",
        "from glob import glob\n",
        "import json\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "def data_summary():\n",
        "  train_list = glob(data_dir['train_data']+'*.png')\n",
        "  test_list = glob(data_dir['test_data']+'*.png')\n",
        "  val_list = glob(data_dir['val_data']+'*.png')\n",
        "  print('train image counts: %d'%len(train_list))\n",
        "  print('val image counts: %d'%len(val_list))\n",
        "  print('test image counts: %d'%len(test_list))\n",
        "\n",
        "data_summary()"
      ],
      "metadata": {
        "id": "v3llQqTedCwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#4.看train数据集第一张的信息，长宽高等\n",
        "def look_train_json():\n",
        "  with open(data_dir['train_label'], 'r', encoding='utf-8') as f:\n",
        "    content = f.read()\n",
        "\n",
        "  content = json.loads(content)\n",
        "\n",
        "  print(content['000000.png'])\n",
        "\n",
        "look_train_json()\n",
        "#5.看需要输出文件的信息\n",
        "def look_submit():\n",
        "  df = pd.read_csv(data_dir['submit_file'], sep=',')\n",
        "  print(df.head(5))\n",
        "\n",
        "look_submit()"
      ],
      "metadata": {
        "id": "QYAChaH1eZo_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#6.统计图片的大小\n",
        "def img_size_summary():\n",
        "  sizes = []\n",
        "\n",
        "  for img in glob(data_dir['train_data']+'*.png'):\n",
        "    img = Image.open(img)\n",
        "\n",
        "    sizes.append(img.size)\n",
        "\n",
        "  sizes = np.array(sizes)\n",
        "\n",
        "  plt.figure(figsize=(10, 8))\n",
        "  plt.scatter(sizes[:, 0], sizes[:, 1])\n",
        "  plt.xlabel('Width')\n",
        "  plt.ylabel('Height')\n",
        "\n",
        "  plt.title('image width-height summary')\n",
        "  plt.show()\n",
        "\n",
        "img_size_summary()"
      ],
      "metadata": {
        "id": "j7BXWuK5eXRs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#7.统计bbox的大小\n",
        "def bbox_summary():\n",
        "  marks = json.loads(open(data_dir['train_label'], 'r').read())\n",
        "  bboxes = []\n",
        "\n",
        "  for img, mark in marks.items():\n",
        "    for i in range(len(mark['label'])):\n",
        "      bboxes.append([mark['left'][i], mark['top'][i], mark['width'][i], mark['height'][i]])\n",
        "\n",
        "  bboxes = np.array(bboxes)\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(12, 8))\n",
        "  ax.scatter(bboxes[:, 2], bboxes[:, 3])\n",
        "  ax.set_title('bbox width-height summary')\n",
        "  ax.set_xlabel('width')\n",
        "  ax.set_ylabel('height')\n",
        "  plt.show()\n",
        "\n",
        "bbox_summary()"
      ],
      "metadata": {
        "id": "APxH_Fy9eLGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#8.统计图片分别含有数字的个数\n",
        "def label_summary():\n",
        "    marks = json.load(open(data_dir['train_label'], 'r'))\n",
        "\n",
        "    dicts = {}\n",
        "    for img, mark in marks.items():\n",
        "        if len(mark['label']) not in dicts:\n",
        "            dicts[len(mark['label'])] = 0\n",
        "        dicts[len(mark['label'])] += 1\n",
        "\n",
        "    dicts = sorted(dicts.items(), key=lambda x: x[0])\n",
        "    for k, v in dicts:\n",
        "        print('%d个数字的图片数目: %d' % (k, v))\n",
        "\n",
        "label_summary()"
      ],
      "metadata": {
        "id": "SxEe0X4HeKGv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T14:23:32.395260Z",
          "start_time": "2020-05-09T14:23:31.939967Z"
        },
        "id": "gtVJOzjST73c"
      },
      "outputs": [],
      "source": [
        "import os, sys, glob, shutil, json\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
        "import cv2\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "\n",
        "%pylab inline\n",
        "\n",
        "import torch\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = False\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data.dataset import Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGU17J-iT73f"
      },
      "source": [
        "# 定义读取数据集"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T14:23:34.383443Z",
          "start_time": "2020-05-09T14:23:34.377373Z"
        },
        "id": "M5BMfe14T73g"
      },
      "outputs": [],
      "source": [
        "class SVHNDataset(Dataset):\n",
        "    def __init__(self, img_path, img_label, transform=None):\n",
        "        self.img_path = img_path\n",
        "        self.img_label = img_label\n",
        "        if transform is not None:\n",
        "            self.transform = transform\n",
        "        else:\n",
        "            self.transform = None\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = Image.open(self.img_path[index]).convert('RGB')\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        lbl = np.array(self.img_label[index], dtype=np.int64)\n",
        "        lbl = list(lbl)  + (5 - len(lbl)) * [10]\n",
        "        return img, torch.from_numpy(np.array(lbl[:5]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Omf3qtDlT73h"
      },
      "source": [
        "# 定义读取数据dataloader\n",
        "\n",
        "假设数据存放在`../input`文件夹下，并进行解压。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T14:23:37.248125Z",
          "start_time": "2020-05-09T14:23:36.943592Z"
        },
        "id": "TrkySTm0T73i"
      },
      "outputs": [],
      "source": [
        "train_path = glob.glob('./NDataset/mchar_train/*.png')\n",
        "train_path.sort()\n",
        "train_json = json.load(open('./NDataset/mchar_train.json'))\n",
        "train_label = [train_json[x]['label'] for x in train_json]\n",
        "print(len(train_path), len(train_label))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    SVHNDataset(train_path, train_label,\n",
        "                transforms.Compose([\n",
        "                    transforms.Resize((64, 128)),\n",
        "                    transforms.RandomCrop((60, 120)),\n",
        "                    transforms.ColorJitter(0.3, 0.3, 0.2),\n",
        "                    transforms.RandomRotation(10),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])),\n",
        "    batch_size=40,\n",
        "    shuffle=True,\n",
        "    num_workers=10,\n",
        ")\n",
        "\n",
        "val_path = glob.glob('./NDataset/mchar_val/*.png')\n",
        "val_path.sort()\n",
        "val_json = json.load(open('./NDataset/mchar_val.json'))\n",
        "val_label = [val_json[x]['label'] for x in val_json]\n",
        "print(len(val_path), len(val_label))\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    SVHNDataset(val_path, val_label,\n",
        "                transforms.Compose([\n",
        "                    transforms.Resize((60, 120)),\n",
        "                    # transforms.ColorJitter(0.3, 0.3, 0.2),\n",
        "                    # transforms.RandomRotation(5),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])),\n",
        "    batch_size=40,\n",
        "    shuffle=False,\n",
        "    num_workers=10,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rqkb-JnT73j"
      },
      "source": [
        "# 定义分类模型\n",
        "\n",
        "这里使用ResNet18的模型进行特征提取"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T14:23:38.370681Z",
          "start_time": "2020-05-09T14:23:38.359476Z"
        },
        "id": "WhBBhP7BT73k"
      },
      "outputs": [],
      "source": [
        "class SVHN_Model1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SVHN_Model1, self).__init__()\n",
        "\n",
        "        model_conv = models.resnet18(pretrained=True)\n",
        "        model_conv.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        model_conv = nn.Sequential(*list(model_conv.children())[:-1])\n",
        "        self.cnn = model_conv\n",
        "\n",
        "        self.fc1 = nn.Linear(512, 11)\n",
        "        self.fc2 = nn.Linear(512, 11)\n",
        "        self.fc3 = nn.Linear(512, 11)\n",
        "        self.fc4 = nn.Linear(512, 11)\n",
        "        self.fc5 = nn.Linear(512, 11)\n",
        "\n",
        "    def forward(self, img):\n",
        "        feat = self.cnn(img)\n",
        "        # print(feat.shape)\n",
        "        feat = feat.view(feat.shape[0], -1)\n",
        "        c1 = self.fc1(feat)\n",
        "        c2 = self.fc2(feat)\n",
        "        c3 = self.fc3(feat)\n",
        "        c4 = self.fc4(feat)\n",
        "        c5 = self.fc5(feat)\n",
        "        return c1, c2, c3, c4, c5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T14:23:39.461245Z",
          "start_time": "2020-05-09T14:23:39.445117Z"
        },
        "id": "58UZCdjOT73l"
      },
      "outputs": [],
      "source": [
        "def train(train_loader, model, criterion, optimizer, epoch):\n",
        "    # 切换模型为训练模式\n",
        "    model.train()\n",
        "    train_loss = []\n",
        "\n",
        "    for i, (input, target) in enumerate(train_loader):\n",
        "        if use_cuda:\n",
        "            input = input.cuda()\n",
        "            target = target.cuda()\n",
        "\n",
        "        c0, c1, c2, c3, c4 = model(input)\n",
        "        loss = criterion(c0, target[:, 0]) + \\\n",
        "                criterion(c1, target[:, 1]) + \\\n",
        "                criterion(c2, target[:, 2]) + \\\n",
        "                criterion(c3, target[:, 3]) + \\\n",
        "                criterion(c4, target[:, 4])\n",
        "\n",
        "        # loss /= 6\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss.append(loss.item())\n",
        "    return np.mean(train_loss)\n",
        "\n",
        "def validate(val_loader, model, criterion):\n",
        "    # 切换模型为预测模型\n",
        "    model.eval()\n",
        "    val_loss = []\n",
        "\n",
        "    # 不记录模型梯度信息\n",
        "    with torch.no_grad():\n",
        "        for i, (input, target) in enumerate(val_loader):\n",
        "            if use_cuda:\n",
        "                input = input.cuda()\n",
        "                target = target.cuda()\n",
        "\n",
        "            c0, c1, c2, c3, c4 = model(input)\n",
        "            loss = criterion(c0, target[:, 0]) + \\\n",
        "                    criterion(c1, target[:, 1]) + \\\n",
        "                    criterion(c2, target[:, 2]) + \\\n",
        "                    criterion(c3, target[:, 3]) + \\\n",
        "                    criterion(c4, target[:, 4])\n",
        "            # loss /= 6\n",
        "            val_loss.append(loss.item())\n",
        "    return np.mean(val_loss)\n",
        "\n",
        "def predict(test_loader, model, tta=10):\n",
        "    model.eval()\n",
        "    test_pred_tta = None\n",
        "\n",
        "    # TTA 次数\n",
        "    for _ in range(tta):\n",
        "        test_pred = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, (input, target) in enumerate(test_loader):\n",
        "                if use_cuda:\n",
        "                    input = input.cuda()\n",
        "\n",
        "                c0, c1, c2, c3, c4 = model(input)\n",
        "                if use_cuda:\n",
        "                    output = np.concatenate([\n",
        "                        c0.data.cpu().numpy(),\n",
        "                        c1.data.cpu().numpy(),\n",
        "                        c2.data.cpu().numpy(),\n",
        "                        c3.data.cpu().numpy(),\n",
        "                        c4.data.cpu().numpy()], axis=1)\n",
        "                else:\n",
        "                    output = np.concatenate([\n",
        "                        c0.data.numpy(),\n",
        "                        c1.data.numpy(),\n",
        "                        c2.data.numpy(),\n",
        "                        c3.data.numpy(),\n",
        "                        c4.data.numpy()], axis=1)\n",
        "\n",
        "                test_pred.append(output)\n",
        "\n",
        "        test_pred = np.vstack(test_pred)\n",
        "        if test_pred_tta is None:\n",
        "            test_pred_tta = test_pred\n",
        "        else:\n",
        "            test_pred_tta += test_pred\n",
        "\n",
        "    return test_pred_tta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzAmy5SlT73n"
      },
      "source": [
        "# 训练与验证"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T14:27:06.642180Z",
          "start_time": "2020-05-09T14:23:50.533281Z"
        },
        "scrolled": true,
        "id": "c9S62sW0T73o"
      },
      "outputs": [],
      "source": [
        "model = SVHN_Model1()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), 0.001)\n",
        "best_loss = 1000.0\n",
        "\n",
        "use_cuda = True\n",
        "if use_cuda:\n",
        "    model = model.cuda()\n",
        "\n",
        "for epoch in range(40):\n",
        "    train_loss = train(train_loader, model, criterion, optimizer, epoch)\n",
        "    val_loss = validate(val_loader, model, criterion)\n",
        "\n",
        "    val_label = [''.join(map(str, x)) for x in val_loader.dataset.img_label]\n",
        "    val_predict_label = predict(val_loader, model, 1)\n",
        "    val_predict_label = np.vstack([\n",
        "        val_predict_label[:, :11].argmax(1),\n",
        "        val_predict_label[:, 11:22].argmax(1),\n",
        "        val_predict_label[:, 22:33].argmax(1),\n",
        "        val_predict_label[:, 33:44].argmax(1),\n",
        "        val_predict_label[:, 44:55].argmax(1),\n",
        "    ]).T\n",
        "    val_label_pred = []\n",
        "    for x in val_predict_label:\n",
        "        val_label_pred.append(''.join(map(str, x[x!=10])))\n",
        "\n",
        "    val_char_acc = np.mean(np.array(val_label_pred) == np.array(val_label))\n",
        "\n",
        "    print('Epoch: {0}, Train loss: {1} \\t Val loss: {2}'.format(epoch, train_loss, val_loss))\n",
        "    print('Val Acc', val_char_acc)\n",
        "    # 记录下验证集精度\n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        # print('Find better model in Epoch {0}, saving model.'.format(epoch))\n",
        "        torch.save(model.state_dict(), './model.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGyFQgUBT73p"
      },
      "source": [
        "# 预测并生成提交文件"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T14:27:46.875945Z",
          "start_time": "2020-05-09T14:27:46.575526Z"
        },
        "id": "egQM-XDhT73p"
      },
      "outputs": [],
      "source": [
        "test_path = glob.glob('./NDataset/mchar_test_a/*.png')\n",
        "test_path.sort()\n",
        "# test_json = json.load(open('./NDataset/mchar_test_a.json'))\n",
        "test_label = [[1]] * len(test_path)\n",
        "print(len(test_path), len(test_label))\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    SVHNDataset(test_path, test_label,\n",
        "                transforms.Compose([\n",
        "                    transforms.Resize((70, 140)),\n",
        "                    # transforms.RandomCrop((60, 120)),\n",
        "                    # transforms.ColorJitter(0.3, 0.3, 0.2),\n",
        "                    # transforms.RandomRotation(5),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])),\n",
        "    batch_size=40,\n",
        "    shuffle=False,\n",
        "    num_workers=10,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T14:27:57.864970Z",
          "start_time": "2020-05-09T14:27:48.691924Z"
        },
        "scrolled": true,
        "id": "f_nwFNXkT73q"
      },
      "outputs": [],
      "source": [
        "# 加载保存的最优模型\n",
        "model.load_state_dict(torch.load('model.pt'))\n",
        "\n",
        "test_predict_label = predict(test_loader, model, 1)\n",
        "print(test_predict_label.shape)\n",
        "\n",
        "test_label = [''.join(map(str, x)) for x in test_loader.dataset.img_label]\n",
        "test_predict_label = np.vstack([\n",
        "    test_predict_label[:, :11].argmax(1),\n",
        "    test_predict_label[:, 11:22].argmax(1),\n",
        "    test_predict_label[:, 22:33].argmax(1),\n",
        "    test_predict_label[:, 33:44].argmax(1),\n",
        "    test_predict_label[:, 44:55].argmax(1),\n",
        "]).T\n",
        "\n",
        "test_label_pred = []\n",
        "for x in test_predict_label:\n",
        "    test_label_pred.append(''.join(map(str, x[x!=10])))\n",
        "\n",
        "import pandas as pd\n",
        "df_submit = pd.read_csv('./NDataset/mchar_test_A_sample_submit.csv')\n",
        "df_submit['file_code'] = test_label_pred\n",
        "df_submit.to_csv('submit.csv', index=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzyN_F5lT73r"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#deepseek优化后的代码"
      ],
      "metadata": {
        "id": "DMzT5QPFFa6u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "class SVHNTrainer:\n",
        "    def __init__(self, model, criterion, optimizer, device='cuda', use_amp=True):\n",
        "        self.model = model.to(device)\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.device = device\n",
        "        self.scaler = GradScaler(enabled=use_amp)\n",
        "\n",
        "        # 训练记录\n",
        "        self.train_losses = []\n",
        "        self.val_losses = []\n",
        "        self.val_accuracies = []\n",
        "        self.best_acc = 0.0\n",
        "\n",
        "    def _compute_loss(self, outputs, targets):\n",
        "        \"\"\"多任务损失计算\"\"\"\n",
        "        return sum(self.criterion(o, targets[:, i]) for i, o in enumerate(outputs))\n",
        "\n",
        "    def _move_data(self, data):\n",
        "        \"\"\"自动数据迁移\"\"\"\n",
        "        if isinstance(data, (list, tuple)):\n",
        "            return [self._move_data(x) for x in data]\n",
        "        return data.to(self.device, non_blocking=True)\n",
        "\n",
        "    def train_epoch(self, train_loader):\n",
        "        self.model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        with tqdm(train_loader, desc=f\"Training\", unit=\"batch\") as pbar:\n",
        "            for inputs, targets in pbar:\n",
        "                # 数据迁移\n",
        "                inputs = self._move_data(inputs)\n",
        "                targets = self._move_data(targets.long())  # 确保标签为long类型\n",
        "\n",
        "                # 梯度清零\n",
        "                self.optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "                # 混合精度训练\n",
        "                with autocast(enabled=self.scaler.is_enabled()):\n",
        "                    outputs = self.model(inputs)\n",
        "                    loss = self._compute_loss(outputs, targets)\n",
        "\n",
        "                # 反向传播\n",
        "                self.scaler.scale(loss).backward()\n",
        "                self.scaler.step(self.optimizer)\n",
        "                self.scaler.update()\n",
        "\n",
        "                # 记录损失\n",
        "                batch_loss = loss.item()\n",
        "                total_loss += batch_loss\n",
        "                pbar.set_postfix({\"loss\": f\"{batch_loss:.4f}\"})\n",
        "\n",
        "        return total_loss / len(train_loader)\n",
        "\n",
        "    def validate(self, val_loader):\n",
        "        self.model.eval()\n",
        "        total_loss = 0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        with torch.no_grad(), tqdm(val_loader, desc=\"Validating\", unit=\"batch\") as pbar:\n",
        "            for inputs, targets in pbar:\n",
        "                inputs = self._move_data(inputs)\n",
        "                targets = self._move_data(targets.long())\n",
        "\n",
        "                outputs = self.model(inputs)\n",
        "                loss = self._compute_loss(outputs, targets)\n",
        "\n",
        "                # 计算准确率\n",
        "                preds = torch.stack([o.argmax(dim=1) for o in outputs], dim=1)\n",
        "                mask = (preds != 10)  # 过滤无效标签\n",
        "                filtered_preds = [p[m] for p, m in zip(preds.permute(1,0), mask.permute(1,0))]\n",
        "                filtered_targets = [t[m] for t, m in zip(targets.permute(1,0), mask.permute(1,0))]\n",
        "\n",
        "                batch_correct = sum((p == t).sum().item() for p, t in zip(filtered_preds, filtered_targets))\n",
        "                batch_total = sum(len(t) for t in filtered_targets)\n",
        "\n",
        "                # 记录指标\n",
        "                batch_loss = loss.item()\n",
        "                total_loss += batch_loss\n",
        "                correct += batch_correct\n",
        "                total += batch_total\n",
        "                pbar.set_postfix({\"loss\": f\"{batch_loss:.4f}\", \"acc\": f\"{batch_correct/batch_total:.2%}\"})\n",
        "\n",
        "        return total_loss/len(val_loader), correct/total if total >0 else 0\n",
        "\n",
        "    def visualize_training(self):\n",
        "        \"\"\"可视化训练过程\"\"\"\n",
        "        plt.figure(figsize=(12, 5))\n",
        "\n",
        "        # 损失曲线\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.plot(self.train_losses, label='Train Loss')\n",
        "        plt.plot(self.val_losses, label='Val Loss')\n",
        "        plt.title('Training and Validation Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        # 准确率曲线\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.plot(self.val_accuracies, label='Val Accuracy', color='orange')\n",
        "        plt.title('Validation Accuracy')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Accuracy')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def predict(self, test_loader, tta=10):\n",
        "        \"\"\"TTA预测优化\"\"\"\n",
        "        self.model.eval()\n",
        "        all_preds = []\n",
        "\n",
        "        for _ in range(tta):\n",
        "            batch_preds = []\n",
        "            with torch.no_grad():\n",
        "                for inputs, _ in test_loader:\n",
        "                    inputs = self._move_data(inputs)\n",
        "                    outputs = self.model(inputs)\n",
        "                    # 在GPU上拼接输出\n",
        "                    pred = torch.stack([o.argmax(dim=1) for o in outputs], dim=1)\n",
        "                    batch_preds.append(pred.cpu())\n",
        "\n",
        "            all_preds.append(torch.cat(batch_preds).numpy())\n",
        "\n",
        "        # 对TTA结果取众数\n",
        "        return np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=0, arr=np.stack(all_preds))\n",
        "\n",
        "# 初始化训练器\n",
        "model = SVHN_Model1()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "trainer = SVHNTrainer(\n",
        "    model=model,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    use_amp=True\n",
        ")\n",
        "\n",
        "# 训练循环\n",
        "EPOCHS = 10\n",
        "patience = 3\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss = trainer.train_epoch(train_loader)\n",
        "    val_loss, val_acc = trainer.validate(val_loader)\n",
        "\n",
        "    # 记录指标\n",
        "    trainer.train_losses.append(train_loss)\n",
        "    trainer.val_losses.append(val_loss)\n",
        "    trainer.val_accuracies.append(val_acc)\n",
        "\n",
        "    # 打印进度\n",
        "    print(f\"Epoch {epoch+1:02d} | \"\n",
        "          f\"Train Loss: {train_loss:.4f} | \"\n",
        "          f\"Val Loss: {val_loss:.4f} | \"\n",
        "          f\"Val Acc: {val_acc:.2%}\")\n",
        "\n",
        "    # 保存最佳模型\n",
        "    if val_acc > trainer.best_acc:\n",
        "        trainer.best_acc = val_acc\n",
        "        torch.save(model.state_dict(), 'best_model.pth')\n",
        "        print(f\"New best model saved with accuracy {val_acc:.2%}\")\n",
        "    else:\n",
        "        print(f\"No improvement in accuracy. Current best accuracy: {trainer.best_acc:.2%}\")\n",
        "\n",
        "    # 可视化\n",
        "    trainer.visualize_training()\n",
        "\n",
        "# 加载最佳模型\n",
        "model.load_state_dict(torch.load('best_model.pth'))\n",
        "\n",
        "# 生成测试结果\n",
        "test_preds = trainer.predict(test_loader, tta=10)"
      ],
      "metadata": {
        "id": "bPsFWAKrFgug"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}