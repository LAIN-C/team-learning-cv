{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TCBAGpD4T73Y"
      },
      "source": [
        "baseline思路：使用CNN进行定长字符分类；\n",
        "\n",
        "运行系统要求：Python2/3，内存4G，有无GPU都可以"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1.下载街景字符识别的数据集，并且解压\n",
        "\n",
        "import pandas as pd\n",
        "import os\n",
        "import requests\n",
        "import zipfile\n",
        "import shutil\n",
        "import shutil\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!ls -l /content/drive/MyDrive\n",
        "\n",
        "links = pd.read_csv('/content/drive/MyDrive/mchar_data_list_0515.csv')\n",
        "dir_name = 'NDataset'\n",
        "mypath = '/content/'\n",
        "if not os.path.exists(mypath + dir_name):\n",
        "    os.mkdir(mypath + dir_name)\n",
        "for i,link in enumerate(links['link']):\n",
        "    file_name = links['file'][i]\n",
        "    print(file_name, '\\t', link)\n",
        "    file_name = mypath + dir_name + '/' + file_name\n",
        "    if not os.path.exists(file_name):\n",
        "        response = requests.get(link, stream=True)\n",
        "        with open( file_name, 'wb') as f:\n",
        "            for chunk in response.iter_content(chunk_size=1024):\n",
        "                if chunk:\n",
        "                    f.write(chunk)\n",
        "zip_list = ['mchar_train', 'mchar_test_a', 'mchar_val']\n",
        "for little_zip in zip_list:\n",
        "    if not os.path.exists(mypath + dir_name + '/' + little_zip):\n",
        "        zip_file = zipfile.ZipFile(mypath + dir_name + '/' + little_zip + '.zip', 'r')\n",
        "        zip_file.extractall(path = mypath + dir_name )\n",
        "if os.path.exists(mypath + dir_name + '/' + '__MACOSX'):\n",
        "    shutil.rmtree(mypath + dir_name + '/' + '__MACOSX')"
      ],
      "metadata": {
        "id": "JpDakhdYV8cP",
        "outputId": "452d97b2-b79c-4b8d-f3de-ad26810c3cd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "total 153553\n",
            "drwx------ 2 root root      4096 Feb 27 15:05 'Colab Notebooks'\n",
            "drwx------ 2 root root      4096 Mar 22 07:35  mchar\n",
            "-rw------- 1 root root       659 Mar 22 07:35  mchar_data_list_0515.csv\n",
            "-rw------- 1 root root 141581580 Mar 19 18:19  model_best.pth\n",
            "-rw------- 1 root root  15646679 Mar 19 17:25  tmp.csv\n",
            "mchar_train.zip \t http://tianchi-competition.oss-cn-hangzhou.aliyuncs.com/531795/mchar_train.zip\n",
            "mchar_train.json \t http://tianchi-competition.oss-cn-hangzhou.aliyuncs.com/531795/mchar_train.json\n",
            "mchar_val.zip \t http://tianchi-competition.oss-cn-hangzhou.aliyuncs.com/531795/mchar_val.zip\n",
            "mchar_val.json \t http://tianchi-competition.oss-cn-hangzhou.aliyuncs.com/531795/mchar_val.json\n",
            "mchar_test_a.zip \t http://tianchi-competition.oss-cn-hangzhou.aliyuncs.com/531795/mchar_test_a.zip\n",
            "mchar_sample_submit_A.csv \t http://tianchi-competition.oss-cn-hangzhou.aliyuncs.com/531795/mchar_sample_submit_A.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2.定义目录路径\n",
        "\n",
        "#目录路径\n",
        "data_dir = {\n",
        "    'train_data': '/content/NDataset/mchar_train/',\n",
        "    'val_data': '/content/NDataset/mchar_val/',\n",
        "    'test_data': '/content/NDataset/mchar_test_a/',\n",
        "    'train_label': '/content/NDataset/mchar_train.json',\n",
        "    'val_label': '/content/NDataset/mchar_val.json',\n",
        "    'submit_file': '/content/NDataset/mchar_sample_submit_A.csv'\n",
        "}\n",
        "3.统计train,val,test数据集的个数\n",
        "\n",
        "#统计train,val,test数据集的个数\n",
        "def data_summary():\n",
        "  train_list = glob(data_dir['train_data']+'*.png')\n",
        "  test_list = glob(data_dir['test_data']+'*.png')\n",
        "  val_list = glob(data_dir['val_data']+'*.png')\n",
        "  print('train image counts: %d'%len(train_list))\n",
        "  print('val image counts: %d'%len(val_list))\n",
        "  print('test image counts: %d'%len(test_list))\n",
        "\n",
        "data_summary()\n",
        "#4.看train数据集第一张的信息，长宽高等\n",
        "\n",
        "#看train数据集第一张的信息，长宽高等\n",
        "def look_train_json():\n",
        "  with open(data_dir['train_label'], 'r', encoding='utf-8') as f:\n",
        "    content = f.read()\n",
        "\n",
        "  content = json.loads(content)\n",
        "\n",
        "  print(content['000000.png'])\n",
        "\n",
        "look_train_json()\n",
        "#5.看需要输出文件的信息\n",
        "def look_submit():\n",
        "  df = pd.read_csv(data_dir['submit_file'], sep=',')\n",
        "  print(df.head(5))\n",
        "\n",
        "look_submit()\n",
        "#6.统计图片的大小\n",
        "def img_size_summary():\n",
        "  sizes = []\n",
        "\n",
        "  for img in glob(data_dir['train_data']+'*.png'):\n",
        "    img = Image.open(img)\n",
        "\n",
        "    sizes.append(img.size)\n",
        "\n",
        "  sizes = np.array(sizes)\n",
        "\n",
        "  plt.figure(figsize=(10, 8))\n",
        "  plt.scatter(sizes[:, 0], sizes[:, 1])\n",
        "  plt.xlabel('Width')\n",
        "  plt.ylabel('Height')\n",
        "\n",
        "  plt.title('image width-height summary')\n",
        "  plt.show()\n",
        "\n",
        "img_size_summary()\n",
        "#7.统计bbox的大小\n",
        "def bbox_summary():\n",
        "  marks = json.loads(open(data_dir['train_label'], 'r').read())\n",
        "  bboxes = []\n",
        "\n",
        "  for img, mark in marks.items():\n",
        "    for i in range(len(mark['label'])):\n",
        "      bboxes.append([mark['left'][i], mark['top'][i], mark['width'][i], mark['height'][i]])\n",
        "\n",
        "  bboxes = np.array(bboxes)\n",
        "\n",
        "  fig, ax = plt.subplots(figsize=(12, 8))\n",
        "  ax.scatter(bboxes[:, 2], bboxes[:, 3])\n",
        "  ax.set_title('bbox width-height summary')\n",
        "  ax.set_xlabel('width')\n",
        "  ax.set_ylabel('height')\n",
        "  plt.show()\n",
        "\n",
        "bbox_summary()\n",
        "#8.统计图片分别含有数字的个数\n",
        "def label_summary():\n",
        "    marks = json.load(open(data_dir['train_label'], 'r'))\n",
        "\n",
        "    dicts = {}\n",
        "    for img, mark in marks.items():\n",
        "        if len(mark['label']) not in dicts:\n",
        "            dicts[len(mark['label'])] = 0\n",
        "        dicts[len(mark['label'])] += 1\n",
        "\n",
        "    dicts = sorted(dicts.items(), key=lambda x: x[0])\n",
        "    for k, v in dicts:\n",
        "        print('%d个数字的图片数目: %d' % (k, v))\n",
        "\n",
        "label_summary()"
      ],
      "metadata": {
        "id": "v3llQqTedCwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T14:23:32.395260Z",
          "start_time": "2020-05-09T14:23:31.939967Z"
        },
        "id": "gtVJOzjST73c",
        "outputId": "c5f5f47e-0ec9-4483-b0cb-bf93d162b5ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        }
      ],
      "source": [
        "import os, sys, glob, shutil, json\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
        "import cv2\n",
        "\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "\n",
        "from tqdm import tqdm, tqdm_notebook\n",
        "\n",
        "%pylab inline\n",
        "\n",
        "import torch\n",
        "torch.manual_seed(0)\n",
        "torch.backends.cudnn.deterministic = False\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "import torchvision.models as models\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data.dataset import Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GGU17J-iT73f"
      },
      "source": [
        "# 定义读取数据集"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T14:23:34.383443Z",
          "start_time": "2020-05-09T14:23:34.377373Z"
        },
        "id": "M5BMfe14T73g"
      },
      "outputs": [],
      "source": [
        "class SVHNDataset(Dataset):\n",
        "    def __init__(self, img_path, img_label, transform=None):\n",
        "        self.img_path = img_path\n",
        "        self.img_label = img_label\n",
        "        if transform is not None:\n",
        "            self.transform = transform\n",
        "        else:\n",
        "            self.transform = None\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        img = Image.open(self.img_path[index]).convert('RGB')\n",
        "\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        lbl = np.array(self.img_label[index], dtype=np.int)\n",
        "        lbl = list(lbl)  + (5 - len(lbl)) * [10]\n",
        "        return img, torch.from_numpy(np.array(lbl[:5]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Omf3qtDlT73h"
      },
      "source": [
        "# 定义读取数据dataloader\n",
        "\n",
        "假设数据存放在`../input`文件夹下，并进行解压。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T14:23:37.248125Z",
          "start_time": "2020-05-09T14:23:36.943592Z"
        },
        "id": "TrkySTm0T73i",
        "outputId": "2da140d6-c55b-4fc8-b1a9-3b6b6baf4c98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '../input/train.json'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-51bd8e072516>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrain_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/train/*.png'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtrain_path\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtrain_json\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../input/train.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mtrain_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtrain_json\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_json\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_label\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../input/train.json'"
          ]
        }
      ],
      "source": [
        "train_path = glob.glob('../input/train/*.png')\n",
        "train_path.sort()\n",
        "train_json = json.load(open('../input/train.json'))\n",
        "train_label = [train_json[x]['label'] for x in train_json]\n",
        "print(len(train_path), len(train_label))\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    SVHNDataset(train_path, train_label,\n",
        "                transforms.Compose([\n",
        "                    transforms.Resize((64, 128)),\n",
        "                    transforms.RandomCrop((60, 120)),\n",
        "                    transforms.ColorJitter(0.3, 0.3, 0.2),\n",
        "                    transforms.RandomRotation(10),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])),\n",
        "    batch_size=40,\n",
        "    shuffle=True,\n",
        "    num_workers=10,\n",
        ")\n",
        "\n",
        "val_path = glob.glob('../input/val/*.png')\n",
        "val_path.sort()\n",
        "val_json = json.load(open('../input/val.json'))\n",
        "val_label = [val_json[x]['label'] for x in val_json]\n",
        "print(len(val_path), len(val_label))\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    SVHNDataset(val_path, val_label,\n",
        "                transforms.Compose([\n",
        "                    transforms.Resize((60, 120)),\n",
        "                    # transforms.ColorJitter(0.3, 0.3, 0.2),\n",
        "                    # transforms.RandomRotation(5),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])),\n",
        "    batch_size=40,\n",
        "    shuffle=False,\n",
        "    num_workers=10,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3rqkb-JnT73j"
      },
      "source": [
        "# 定义分类模型\n",
        "\n",
        "这里使用ResNet18的模型进行特征提取"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T14:23:38.370681Z",
          "start_time": "2020-05-09T14:23:38.359476Z"
        },
        "id": "WhBBhP7BT73k"
      },
      "outputs": [],
      "source": [
        "class SVHN_Model1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SVHN_Model1, self).__init__()\n",
        "\n",
        "        model_conv = models.resnet18(pretrained=True)\n",
        "        model_conv.avgpool = nn.AdaptiveAvgPool2d(1)\n",
        "        model_conv = nn.Sequential(*list(model_conv.children())[:-1])\n",
        "        self.cnn = model_conv\n",
        "\n",
        "        self.fc1 = nn.Linear(512, 11)\n",
        "        self.fc2 = nn.Linear(512, 11)\n",
        "        self.fc3 = nn.Linear(512, 11)\n",
        "        self.fc4 = nn.Linear(512, 11)\n",
        "        self.fc5 = nn.Linear(512, 11)\n",
        "\n",
        "    def forward(self, img):\n",
        "        feat = self.cnn(img)\n",
        "        # print(feat.shape)\n",
        "        feat = feat.view(feat.shape[0], -1)\n",
        "        c1 = self.fc1(feat)\n",
        "        c2 = self.fc2(feat)\n",
        "        c3 = self.fc3(feat)\n",
        "        c4 = self.fc4(feat)\n",
        "        c5 = self.fc5(feat)\n",
        "        return c1, c2, c3, c4, c5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T14:23:39.461245Z",
          "start_time": "2020-05-09T14:23:39.445117Z"
        },
        "id": "58UZCdjOT73l"
      },
      "outputs": [],
      "source": [
        "def train(train_loader, model, criterion, optimizer, epoch):\n",
        "    # 切换模型为训练模式\n",
        "    model.train()\n",
        "    train_loss = []\n",
        "\n",
        "    for i, (input, target) in enumerate(train_loader):\n",
        "        if use_cuda:\n",
        "            input = input.cuda()\n",
        "            target = target.cuda()\n",
        "\n",
        "        c0, c1, c2, c3, c4 = model(input)\n",
        "        loss = criterion(c0, target[:, 0]) + \\\n",
        "                criterion(c1, target[:, 1]) + \\\n",
        "                criterion(c2, target[:, 2]) + \\\n",
        "                criterion(c3, target[:, 3]) + \\\n",
        "                criterion(c4, target[:, 4])\n",
        "\n",
        "        # loss /= 6\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss.append(loss.item())\n",
        "    return np.mean(train_loss)\n",
        "\n",
        "def validate(val_loader, model, criterion):\n",
        "    # 切换模型为预测模型\n",
        "    model.eval()\n",
        "    val_loss = []\n",
        "\n",
        "    # 不记录模型梯度信息\n",
        "    with torch.no_grad():\n",
        "        for i, (input, target) in enumerate(val_loader):\n",
        "            if use_cuda:\n",
        "                input = input.cuda()\n",
        "                target = target.cuda()\n",
        "\n",
        "            c0, c1, c2, c3, c4 = model(input)\n",
        "            loss = criterion(c0, target[:, 0]) + \\\n",
        "                    criterion(c1, target[:, 1]) + \\\n",
        "                    criterion(c2, target[:, 2]) + \\\n",
        "                    criterion(c3, target[:, 3]) + \\\n",
        "                    criterion(c4, target[:, 4])\n",
        "            # loss /= 6\n",
        "            val_loss.append(loss.item())\n",
        "    return np.mean(val_loss)\n",
        "\n",
        "def predict(test_loader, model, tta=10):\n",
        "    model.eval()\n",
        "    test_pred_tta = None\n",
        "\n",
        "    # TTA 次数\n",
        "    for _ in range(tta):\n",
        "        test_pred = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for i, (input, target) in enumerate(test_loader):\n",
        "                if use_cuda:\n",
        "                    input = input.cuda()\n",
        "\n",
        "                c0, c1, c2, c3, c4 = model(input)\n",
        "                if use_cuda:\n",
        "                    output = np.concatenate([\n",
        "                        c0.data.cpu().numpy(),\n",
        "                        c1.data.cpu().numpy(),\n",
        "                        c2.data.cpu().numpy(),\n",
        "                        c3.data.cpu().numpy(),\n",
        "                        c4.data.cpu().numpy()], axis=1)\n",
        "                else:\n",
        "                    output = np.concatenate([\n",
        "                        c0.data.numpy(),\n",
        "                        c1.data.numpy(),\n",
        "                        c2.data.numpy(),\n",
        "                        c3.data.numpy(),\n",
        "                        c4.data.numpy()], axis=1)\n",
        "\n",
        "                test_pred.append(output)\n",
        "\n",
        "        test_pred = np.vstack(test_pred)\n",
        "        if test_pred_tta is None:\n",
        "            test_pred_tta = test_pred\n",
        "        else:\n",
        "            test_pred_tta += test_pred\n",
        "\n",
        "    return test_pred_tta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VzAmy5SlT73n"
      },
      "source": [
        "# 训练与验证"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T14:27:06.642180Z",
          "start_time": "2020-05-09T14:23:50.533281Z"
        },
        "scrolled": true,
        "id": "c9S62sW0T73o",
        "outputId": "f944a573-58ba-4a81-afc7-6f5e35f03884"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 0, Train loss: 3.549230361620585 \t Val loss: 3.4495382709503173\n",
            "0.3484\n",
            "Epoch: 1, Train loss: 2.262838746547699 \t Val loss: 3.070304814338684\n",
            "0.4262\n",
            "Epoch: 2, Train loss: 1.9043410422801972 \t Val loss: 2.8098975682258605\n",
            "0.4742\n",
            "Epoch: 3, Train loss: 1.6968964731693268 \t Val loss: 2.6710116691589354\n",
            "0.4948\n",
            "Epoch: 4, Train loss: 1.5359156634807587 \t Val loss: 2.652583309173584\n",
            "0.5126\n",
            "Epoch: 5, Train loss: 1.4319444489479065 \t Val loss: 2.630811668395996\n",
            "0.5247\n",
            "Epoch: 6, Train loss: 1.3359439101219177 \t Val loss: 2.477479434013367\n",
            "0.5383\n",
            "Epoch: 7, Train loss: 1.244316361983617 \t Val loss: 2.458457417488098\n",
            "0.5498\n",
            "Epoch: 8, Train loss: 1.182295233209928 \t Val loss: 2.5932382040023803\n",
            "0.5316\n",
            "Epoch: 9, Train loss: 1.1050671869913737 \t Val loss: 2.4330560541152955\n",
            "0.554\n"
          ]
        }
      ],
      "source": [
        "model = SVHN_Model1()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), 0.001)\n",
        "best_loss = 1000.0\n",
        "\n",
        "use_cuda = True\n",
        "if use_cuda:\n",
        "    model = model.cuda()\n",
        "\n",
        "for epoch in range(10):\n",
        "    train_loss = train(train_loader, model, criterion, optimizer, epoch)\n",
        "    val_loss = validate(val_loader, model, criterion)\n",
        "\n",
        "    val_label = [''.join(map(str, x)) for x in val_loader.dataset.img_label]\n",
        "    val_predict_label = predict(val_loader, model, 1)\n",
        "    val_predict_label = np.vstack([\n",
        "        val_predict_label[:, :11].argmax(1),\n",
        "        val_predict_label[:, 11:22].argmax(1),\n",
        "        val_predict_label[:, 22:33].argmax(1),\n",
        "        val_predict_label[:, 33:44].argmax(1),\n",
        "        val_predict_label[:, 44:55].argmax(1),\n",
        "    ]).T\n",
        "    val_label_pred = []\n",
        "    for x in val_predict_label:\n",
        "        val_label_pred.append(''.join(map(str, x[x!=10])))\n",
        "\n",
        "    val_char_acc = np.mean(np.array(val_label_pred) == np.array(val_label))\n",
        "\n",
        "    print('Epoch: {0}, Train loss: {1} \\t Val loss: {2}'.format(epoch, train_loss, val_loss))\n",
        "    print('Val Acc', val_char_acc)\n",
        "    # 记录下验证集精度\n",
        "    if val_loss < best_loss:\n",
        "        best_loss = val_loss\n",
        "        # print('Find better model in Epoch {0}, saving model.'.format(epoch))\n",
        "        torch.save(model.state_dict(), './model.pt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGyFQgUBT73p"
      },
      "source": [
        "# 预测并生成提交文件"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T14:27:46.875945Z",
          "start_time": "2020-05-09T14:27:46.575526Z"
        },
        "id": "egQM-XDhT73p",
        "outputId": "98db8e81-bd5a-4194-bdfb-199240177f18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "40000 40000\n"
          ]
        }
      ],
      "source": [
        "test_path = glob.glob('../input/test_a/*.png')\n",
        "test_path.sort()\n",
        "test_json = json.load(open('../input/test_a.json'))\n",
        "test_label = [[1]] * len(test_path)\n",
        "print(len(test_path), len(test_label))\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    SVHNDataset(test_path, test_label,\n",
        "                transforms.Compose([\n",
        "                    transforms.Resize((70, 140)),\n",
        "                    # transforms.RandomCrop((60, 120)),\n",
        "                    # transforms.ColorJitter(0.3, 0.3, 0.2),\n",
        "                    # transforms.RandomRotation(5),\n",
        "                    transforms.ToTensor(),\n",
        "                    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "    ])),\n",
        "    batch_size=40,\n",
        "    shuffle=False,\n",
        "    num_workers=10,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2020-05-09T14:27:57.864970Z",
          "start_time": "2020-05-09T14:27:48.691924Z"
        },
        "scrolled": true,
        "id": "f_nwFNXkT73q",
        "outputId": "3df3ae6d-9289-4dc5-e0ec-dc7e6adaaa67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(40000, 55)\n"
          ]
        }
      ],
      "source": [
        "# 加载保存的最优模型\n",
        "model.load_state_dict(torch.load('model.pt'))\n",
        "\n",
        "test_predict_label = predict(test_loader, model, 1)\n",
        "print(test_predict_label.shape)\n",
        "\n",
        "test_label = [''.join(map(str, x)) for x in test_loader.dataset.img_label]\n",
        "test_predict_label = np.vstack([\n",
        "    test_predict_label[:, :11].argmax(1),\n",
        "    test_predict_label[:, 11:22].argmax(1),\n",
        "    test_predict_label[:, 22:33].argmax(1),\n",
        "    test_predict_label[:, 33:44].argmax(1),\n",
        "    test_predict_label[:, 44:55].argmax(1),\n",
        "]).T\n",
        "\n",
        "test_label_pred = []\n",
        "for x in test_predict_label:\n",
        "    test_label_pred.append(''.join(map(str, x[x!=10])))\n",
        "\n",
        "import pandas as pd\n",
        "df_submit = pd.read_csv('../input/test_A_sample_submit.csv')\n",
        "df_submit['file_code'] = test_label_pred\n",
        "df_submit.to_csv('submit.csv', index=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzyN_F5lT73r"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}